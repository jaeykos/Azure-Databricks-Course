{"cells":[{"cell_type":"markdown","source":["# Reading and Writing to Synapse\n\n## Learning Objectives\nBy the end of this lesson, you should be able to:\n* Describe the connection architecture of Synapse and Spark\n* Configure a connection between Databricks and Synapse\n* Read data from Synapse\n* Write data to Synapse\n\n### Azure Synapse\n- leverages massively parallel processing (MPP) to quickly run complex queries across petabytes of data\n- PolyBase T-SQL queries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa92bc09-baa0-4672-9fc0-5a257c89a6c9"}}},{"cell_type":"markdown","source":["## Synapse Connector\n- uses Azure Blob Storage as intermediary\n- uses PolyBase in Synapse\n- enables MPP reads and writes to Synapse from Azure Databricks\n\nNote: The Synapse connector is more suited to ETL than to interactive queries. For interactive and ad-hoc queries, data should be extracted into a Databricks Delta table.\n\n```\n                           ┌─────────┐\n      ┌───────────────────>│ STORAGE │<──────────────────┐\n      │ Storage acc key /  │ ACCOUNT │ Storage acc key / │\n      │ Managed Service ID └─────────┘ OAuth 2.0         │\n      │                         │                        │\n      │                         │ Storage acc key /      │\n      │                         │ OAuth 2.0              │\n      v                         v                 ┌──────v────┐\n┌──────────┐              ┌──────────┐            │┌──────────┴┐\n│ Synapse  │              │  Spark   │            ││ Spark     │\n│ Analytics│<────────────>│  Driver  │<───────────>| Executors │\n└──────────┘  JDBC with   └──────────┘ Configured  └───────────┘\n              username &               in Spark\n              password\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eedb6349-7857-4f21-883d-28e9fc821b63"}}},{"cell_type":"markdown","source":["## SQL DW Connection\n\nThree connections are made to exchange queries and data between Databricks and Synapse\n1. **Spark driver to Synapse**\n   - the Spark driver connects to Synapse via JDBC using a username and password\n2. **Spark driver and executors to Azure Blob Storage**\n   - the Azure Blob Storage container acts as an intermediary to store bulk data when reading from or writing to Synapse\n   - Spark connects to the Blob Storage container using the Azure Blob Storage connector bundled in Databricks Runtime\n   - the URI scheme for specifying this connection must be wasbs\n   - the credential used for setting up this connection must be a storage account access key\n   - the account access key is set in the session configuration associated with the notebook that runs the command\n   - this configuration does not affect other notebooks attached to the same cluster. `spark` is the SparkSession object provided in the notebook\n3. **Synapse to Azure Blob Storage**\n   - Synapse also connects to the Blob Storage container during loading and unloading of temporary data\n   - set `forwardSparkAzureStorageCredentials` to true\n   - the forwarded storage access key is represented by a temporary database scoped credential in the Synapse instance\n   - Synapse connector creates a database scoped credential before asking Synapse to load or unload data\n   - then it deletes the database scoped credential once the loading or unloading operation is done."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c57b1284-80c4-4b62-a603-75d177ca45b0"}}},{"cell_type":"markdown","source":["## Enter Variables from Cloud Setup\n\nBefore starting this lesson, you were guided through configuring Azure Synapse and deploying a Storage Account and blob container.\n\nIn the cell below, enter the **Storage Account Name**, the **Container Name**, and the **Access Key** for the blob container you created.\n\nAlso enter the JDBC connection string for your Azure Synapse instance. Make sure you substitute in your password as indicated within the generated string."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76ecf6e5-1827-47f2-a873-1307f7c190bd"}}},{"cell_type":"code","source":["storageAccount = \"name-of-your-storage-account\"\ncontainerName = \"data\"\naccessKey = \"your-storage-key\"\njdbcURI = \"\"\n\nspark.conf.set(f\"fs.azure.account.key.{storageAccount}.blob.core.windows.net\", accessKey)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb77d108-ae72-49ec-907c-a49c8083ce0b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Read from the Customer Table\n\nNext, use the Synapse Connector to read data from the Customer Table.\n\nUse the read to define a tempory table that can be queried.\n\nNote:\n\n- the connector uses a caching directory on the Azure Blob Container.\n- `forwardSparkAzureStorageCredentials` is set to `true` so that the Synapse instance can access the blob for its MPP read via Polybase"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec965f75-c9cb-410d-ba85-15d55ef6f935"}}},{"cell_type":"code","source":["cacheDir = f\"wasbs://{containerName}@{storageAccount}.blob.core.windows.net/cacheDir\"\n\ntableName = \"dbo.DimCustomer\"\n\ncustomerDF = (spark.read\n  .format(\"com.databricks.spark.sqldw\")\n  .option(\"url\", jdbcURI)\n  .option(\"tempDir\", cacheDir)\n  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n  .option(\"dbTable\", tableName)\n  .load())\n\ncustomerDF.createOrReplaceTempView(\"customer_data\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f3082ae-80c4-4cf9-b33d-2d1e7e815147"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Use SQL queries to count the number of rows in the Customer table and to display table metadata."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27f6612d-7ccd-45d9-8d83-5034dbd58f94"}}},{"cell_type":"code","source":["%sql\nselect count(*) from customer_data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96b2d6eb-35a5-434f-b2ae-e5cac0897cee"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\ndescribe customer_data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f4d6d4b-9f43-4fd0-93fc-7efa3668edc1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that `CustomerKey` and `CustomerAlternateKey` use a very similar naming convention."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6fd8908d-c37b-4220-a172-eae5bb543607"}}},{"cell_type":"code","source":["%sql\nselect CustomerKey, CustomerAlternateKey from customer_data limit 10;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a2e0065e-4840-47e9-a45f-f685f29b7a0b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In a situation in which we may be merging many new customers into this table, we can imagine that we may have issues with uniqueness with regard to the `CustomerKey`. Let us redefine `CustomerAlternateKey` for stronger uniqueness using a [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).\n\nTo do this we will define a UDF and use it to transform the `CustomerAlternateKey` column. Once this is done, we will write the updated Customer Table to a Staging table.\n\n**Note:** It is a best practice to update the Synapse instance via a staging table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9f249a3-42c6-4c59-9707-02c5055eff95"}}},{"cell_type":"code","source":["import uuid\n\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.functions import udf\n\nuuidUdf = udf(lambda : str(uuid.uuid4()), StringType())\ncustomerUpdatedDF = customerDF.withColumn(\"CustomerAlternateKey\", uuidUdf())\ndisplay(customerUpdatedDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c3ddcb7d-9c22-4e7c-b29a-e05f16e40778"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Use the Polybase Connector to Write to the Staging Table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d5a4035-b73d-4100-8259-7ca01db2a28b"}}},{"cell_type":"code","source":["(customerUpdatedDF.write\n  .format(\"com.databricks.spark.sqldw\")\n  .mode(\"overwrite\")\n  .option(\"url\", jdbcURI)\n  .option(\"forward_spark_azure_storage_credentials\", \"true\")\n  .option(\"dbtable\", tableName + \"Staging\")\n  .option(\"tempdir\", cacheDir)\n  .save())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4dff16c-92ba-4f8b-896c-9ca8aae79c69"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Read and Display Changes from Staging Table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8627e807-b1df-47d0-b5aa-61bf3b280332"}}},{"cell_type":"code","source":["customerTempDF = (spark.read\n  .format(\"com.databricks.spark.sqldw\")\n  .option(\"url\", jdbcURI)\n  .option(\"tempDir\", cacheDir)\n  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n  .option(\"dbTable\", tableName + \"Staging\")\n  .load())\n\ncustomerTempDF.createOrReplaceTempView(\"customer_temp_data\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a230cdf-f6c6-45ea-b1c0-b5d52351b75e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nselect CustomerKey, CustomerAlternateKey from customer_temp_data limit 10;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"135d3d9e-89c8-4c14-a190-182bf48ca973"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"1.Azure-Synapse-Analytics","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2935649372840847}},"nbformat":4,"nbformat_minor":0}
